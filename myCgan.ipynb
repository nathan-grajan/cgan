{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nathan-grajan/cgan/blob/main/myCgan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bKzm9u5Gtn_o",
        "outputId": "45c52b9f-7e7e-429b-fef6-0102e4b44bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import torch \n",
        "from torchvision import datasets, transforms\n",
        "import numpy as np\n",
        "import torch.nn.utils\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision.utils import save_image, make_grid\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.autograd import Variable\n",
        "print(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQWfuxnSt-tX",
        "outputId": "c3b60635-299d-488d-9874-e0eb06b8d6f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 144095932.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 104879388.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 43222700.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4763823.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([100, 1, 64, 64])\n"
          ]
        }
      ],
      "source": [
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,)), transforms.Resize(64)\n",
        "                              ])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('./data', download=True, train=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True)\n",
        "\n",
        "# Download and load the test data\n",
        "testset = datasets.MNIST('./data', download=True, train=False, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=True)\n",
        "\n",
        "dataiter = iter(trainloader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "print(images.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kev6VpaB8En6"
      },
      "outputs": [],
      "source": [
        "# IMPLEMENTATION \n",
        "# class Generator(nn.Module):\n",
        "#     def __init__(self, latent_dim = 100, n_classes = 10, img_shape = 28):\n",
        "#         super(Generator, self).__init__()\n",
        "\n",
        "#         self.label_emb = nn.Embedding(n_classes, n_classes)\n",
        "#         self.img_shape=img_shape\n",
        "#         def block(in_feat, out_feat, normalize=True):\n",
        "#             layers = [nn.Linear(in_feat, out_feat)]\n",
        "#             if normalize:\n",
        "#                 layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "#             layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "#             return layers\n",
        "\n",
        "#         self.model = nn.Sequential(\n",
        "#             *block(latent_dim + n_classes, 128, normalize=False),\n",
        "#             *block(128, 256),\n",
        "#             *block(256, 512),\n",
        "#             *block(512, 1024),\n",
        "#             nn.Linear(1024, self.img_shape**2),\n",
        "#             nn.Tanh()\n",
        "#         )\n",
        "\n",
        "#     def forward(self, z, y):\n",
        "#         y = y.int()\n",
        "#         # Concatenate label embedding and image to produce input\n",
        "#         gen_input = torch.cat((self.label_emb(y).view(100,-1), z), -1)\n",
        "#         img = self.model(gen_input)\n",
        "#         img = img.view(img.size(0), 1, self.img_shape, self.img_shape)\n",
        "#         return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VvcZmgLd1e4I"
      },
      "outputs": [],
      "source": [
        "# # IMPLEMENTATION\n",
        "\n",
        "\n",
        "# class Discriminator(nn.Module):\n",
        "#     def __init__(self, n_classes=10, img_shape=28):\n",
        "#         super(Discriminator, self).__init__()\n",
        "\n",
        "#         self.label_embedding = nn.Embedding(n_classes, n_classes)\n",
        "\n",
        "#         self.model = nn.Sequential(\n",
        "#             nn.Linear(n_classes + 28**2, 512),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "#             nn.Linear(512, 512),\n",
        "#             nn.Dropout(0.4),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "#             nn.Linear(512, 512),\n",
        "#             nn.Dropout(0.4),\n",
        "#             nn.LeakyReLU(0.2, inplace=True),\n",
        "#             nn.Linear(512, 1),\n",
        "#         )\n",
        "\n",
        "#     def forward(self, img, labels):\n",
        "#         labels = labels.int()\n",
        "#         # Concatenate label embedding and image to produce input\n",
        "#         d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels).view(100,-1)), -1)\n",
        "#         validity = self.model(d_in)\n",
        "#         return validity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IS52AXFT6iW0"
      },
      "outputs": [],
      "source": [
        "# DCGAN ATTEMPT \n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, dim_z=100, img_size=64, num_channels=1, num_classes=10, embedding_dim = 10):\n",
        "        super(Generator, self).__init__()\n",
        "        self.dim_z = dim_z\n",
        "        self.img_size = img_size\n",
        "        self.num_channels = num_channels\n",
        "        self.num_classes = num_classes\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.label_embedding = nn.Embedding(self.num_classes, embedding_dim=self.embedding_dim)\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            # INPUT channel x 2 x 2\n",
        "            nn.ConvTranspose2d(self.dim_z + self.embedding_dim, self.img_size * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(self.img_size * 8),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (channel*8) x 4 x 4\n",
        "            nn.ConvTranspose2d(self.img_size * 8, self.img_size * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.img_size * 4),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (channel*4) x 8 x 8\n",
        "            nn.ConvTranspose2d(self.img_size * 4, self.img_size * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.img_size * 2),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (channel*2) x 16 x 16\n",
        "            nn.ConvTranspose2d(self.img_size * 2, self.img_size, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.img_size),\n",
        "            nn.ReLU(True),\n",
        "            # state size. (channel) x 32 x 32\n",
        "        \n",
        "\n",
        "            nn.ConvTranspose2d(self.img_size, self.num_channels, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "            \n",
        "            # state size. (nc) x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, z, labels):\n",
        "        z = z.reshape(-1, self.dim_z, 1, 1)  # MAKE 4D FOR transpose (b_size X dim_z X 1 X 1)\n",
        "        labels = labels.int() \n",
        "        label_emb = self.label_embedding(labels).reshape(labels.shape[0], self.embedding_dim, 1, 1) #  100 x 10 X 1 X 1\n",
        "        concat = torch.cat((label_emb, z), 1)\n",
        "        out = self.model(concat).view(-1, self.num_channels, self.img_size, self.img_size)\n",
        "\n",
        "        return out  # should be num_channels x img_size * img_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3dMvRD2535A"
      },
      "outputs": [],
      "source": [
        "# DCGAN ATTEMPT\n",
        "\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, z_dim=100, num_channels=1, img_size=64, num_classes=10, embedding_dim=10):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.z_dim = z_dim\n",
        "        self.num_channels = num_channels\n",
        "        self.img_size = img_size\n",
        "        self.num_classes = num_classes\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "\n",
        "        self.label_transform = nn.Sequential(\n",
        "            nn.Embedding(self.num_classes, self.embedding_dim),\n",
        "            nn.Linear(self.embedding_dim, 1 * 64 * 64) # change it to 100 X 64**2\n",
        "        )\n",
        "        self.model = nn.Sequential(\n",
        "            # NEED TO DO SPECTRAL NORM AFTER CONVOLUTIONS\n",
        "            # input is (num_channels + 1) x 64 x 64\n",
        "            nn.Conv2d(self.num_channels + 1, self.img_size, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # state size. (img_size) x 32 x 32\n",
        "            nn.Conv2d(self.img_size, self.img_size * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.img_size * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # state size. (img_size*2 (128)) x 16 x 16\n",
        "            nn.Conv2d(self.img_size * 2, self.img_size * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.img_size * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            # state size. (img_size*4 (256)) x 8 x 8\n",
        "            nn.Conv2d(self.img_size * 4, self.img_size * 8, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(self.img_size * 8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "            # state size. (ndf*8) x 4 x 4\n",
        "            nn.Conv2d(self.img_size * 8, 1, 5, 2, 1, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, img, labels):\n",
        "        # img should be input B * 1 * img_size * img_size\n",
        "        labels = labels.int() \n",
        "        label_emb = self.label_transform(labels).reshape(labels.shape[0], self.num_channels, self.img_size, self.img_size) #  b_size X 1 X 64 X 64\n",
        "\n",
        "        concat = torch.cat((label_emb, img), 1)\n",
        "        out = self.model(concat)\n",
        "\n",
        "        return out.squeeze() # should be a scalar, may need to squeeze\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwbZbtA9azkz"
      },
      "outputs": [],
      "source": [
        "def sample_image(n_row, batches_done):\n",
        "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
        "    # Sample noise\n",
        "\n",
        "    batch_size, latent_dim = 100, 100\n",
        "    with torch.no_grad():\n",
        "      z = torch.rand(batch_size, latent_dim).to(device)\n",
        "      # Get labels ranging from 0 to n_classes for n rows\n",
        "      labels = np.array([num for _ in range(n_row) for num in range(n_row)])\n",
        "      labels = Variable(torch.LongTensor(labels)).to(device)\n",
        "      gen_imgs = generator(z, labels)\n",
        "\n",
        "      save_image(gen_imgs.data, f\"/content/images{batches_done}.png\", nrow=n_row, normalize=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# custom weights initialization called on ``netG`` and ``netD``\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)"
      ],
      "metadata": {
        "id": "7YYdG1YR_8eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gL03Lh1DIAnI"
      },
      "outputs": [],
      "source": [
        "# TRAINING LOOP\n",
        "\n",
        "\n",
        "generator = Generator().to(device).apply(weights_init)\n",
        "discriminator = Discriminator().to(device).apply(weights_init)\n",
        "# Learning rate for optimizers\n",
        "lr = 0.0002\n",
        "\n",
        "# Beta1 hyperparameter for Adam optimizers\n",
        "beta1 = 0.5\n",
        "\n",
        "\n",
        "optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas = (beta1, 0.999))\n",
        "optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas = (beta1, 0.999))\n",
        "# lr_scheduler_G = torch.optim.lr_scheduler.ExponentialLR(optimizer_G, gamma=1.00004)\n",
        "# lr_scheduler_D = torch.optim.lr_scheduler.ExponentialLR(optimizer_D, gamma=1.00004) # decays every epoch\n",
        "\n",
        "loss = nn.BCELoss()\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 100\n",
        "decay = 1.00004\n",
        "dropout_probability = 0.5\n",
        "num_epochs = 50\n",
        "latent_dim = 100\n",
        "num_classes = 10 # MNIST\n",
        "\n",
        "# ALL LABEL VARIABLES NEED TO BE (BATCHSIZE X 1)\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (data_images, data_labels) in enumerate(trainloader, 0):\n",
        "        data_labels = data_labels.reshape(-1,1).float()\n",
        "        real_labels = torch.ones((batch_size), dtype=torch.float32).to(device) # 100 X 1 label for each sample in batch\n",
        "        fake_labels = torch.zeros((batch_size), dtype=torch.float32).to(device)\n",
        "        data_images = data_images.to(device)\n",
        "        data_labels = data_labels.to(device)\n",
        "\n",
        "\n",
        "        ############################\n",
        "        # (1) Update G network\n",
        "        ###########################\n",
        "        # First train generator because we need fake images for discriminator\n",
        "        optimizer_G.zero_grad()\n",
        "        generator.zero_grad()\n",
        "        noise = torch.rand(batch_size, latent_dim).to(device)\n",
        "        rand_labels = torch.randint(low=0, high=num_classes, size=(batch_size,), dtype=torch.float32).to(device)\n",
        "        # scale the noise vector to the unit hypercube\n",
        "        #noise = 2 * noise - 1\n",
        "        gen_images = generator(noise, rand_labels)\n",
        "\n",
        "        validity = discriminator(gen_images, rand_labels)\n",
        "        gen_loss = loss(validity, real_labels)\n",
        "        gen_loss.backward()\n",
        "        optimizer_G.step()\n",
        "        \n",
        "\n",
        "        ############################\n",
        "        # (2) Update D network\n",
        "        ###########################\n",
        "\n",
        "\n",
        "\n",
        "        optimizer_D.zero_grad()\n",
        "        discriminator.zero_grad()\n",
        "\n",
        "        validity_real = discriminator(data_images, data_labels)\n",
        "        d_real_loss = loss(validity_real, real_labels)\n",
        "\n",
        "        validity_fake = discriminator(gen_images.detach(), rand_labels)\n",
        "        d_fake_loss = loss(validity_fake, fake_labels)\n",
        "\n",
        "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        print(\n",
        "            \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
        "            % (epoch, num_epochs, i, len(trainloader), d_loss.item(), gen_loss.item())\n",
        "        )\n",
        "\n",
        "        batches_done = epoch * len(trainloader) + i\n",
        "        if batches_done % 500 == 0:\n",
        "            sample_image(n_row=10, batches_done=batches_done)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGGSEHgAJIGm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-N4AskX9hu6"
      },
      "outputs": [],
      "source": [
        "#TESTING LOOP\n",
        "generator.eval() # set generator to evaluation mode\n",
        "num_samples = 1000 # number of samples to generate\n",
        "batch_size = 100 # batch size for generating samples\n",
        "\n",
        "\n",
        "#create a fixed noise vector for generating samples\n",
        "fixed_noise = torch.randn(batch_size, latent_dim).to(device)\n",
        "fixed_labels = torch.randint(low=0, high=num_classes, size=(batch_size,), dtype=torch.float32).to(device)\n",
        "\n",
        "#generate samples\n",
        "generated_samples = []\n",
        "with torch.no_grad():\n",
        "  for i in range(num_samples // batch_size):\n",
        "    noise = fixed_noise\n",
        "    labels = fixed_labels\n",
        "    fake_images = generator(noise, labels)\n",
        "    generated_samples.append(fake_images.cpu().detach())\n",
        "\n",
        "    generated_samples = torch.cat(generated_samples, dim=0)\n",
        "\n",
        "   \n",
        "    sample_image(n_row=10, batches_done=0, images=generated_samples) # use the same sample_image function from the training loop"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNdaKPNMEfE1X3z3fYYhBAm",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}